---
title: "Cramming More Components onto Integrated Circuits"
authors: "Gordon E. Moore"
venue: "Electronics Magazine, Vol. 38, No. 8"
year: 1965
---

# Cramming More Components onto Integrated Circuits

## 基本信息

- **发表**：Electronics Magazine, Volume 38, Number 8, April 19, 1965
- **作者**：Gordon E. Moore，时任 Fairchild Semiconductor 研发实验室主任，后为 Intel 联合创始人

## 一句话总结

> 首次提出集成电路上元件数目每年翻倍的经验性预测，即后世所称的 "Moore's Law"。

## 问题与动机

1965年，集成电路技术刚刚从实验室走向工程实践。Moore 在这篇面向工业界的前瞻性文章中试图回答一个核心问题：**集成电路的集成度将以怎样的速度增长，这种增长的极限和经济驱动力是什么？**

这篇文章的写作背景和动机包括：

- 当时集成电路已经在军事系统（如 Apollo 登月计划）中证明了其可靠性，商用计算机也开始采用集成电路，但业界对其未来发展路径缺乏系统性的定量预判。
- 半导体集成电路、薄膜技术、微组装技术等多条路线并存，Moore 需要为 semiconductor integrated circuit 这条路线提供一个令人信服的长期愿景。
- 制造成本与集成度之间存在一个 trade-off：增加单芯片元件数可以降低单元件成本，但良率下降又会推高成本。如何理解这个动态平衡，是指导产业投资的关键问题。

## 核心方法

### 关键思路

Moore 并非提出一个"方法"或"算法"，而是基于对 1959–1965 年间集成电路产业数据的经验观察，做出了一个定量外推：**最低成本对应的单芯片元件数，大约以每年翻一倍的速率增长**。他进一步论证了这一趋势在未来至少 10 年（到 1975 年）内没有根本性的物理或工程障碍。

关键洞察在于：Moore 没有孤立地看技术能力，而是将**经济性（cost per component）作为核心度量**——不是问"能做多少"，而是问"做多少最划算"。

### 技术细节

**成本模型分析**

Moore 构建了一个定性的成本分析框架：

- 对于简单电路，单元件成本近似与元件数成反比——因为同样大小的 die 和同样的封装可以容纳更多功能。
- 但随着元件数增加，良率（yield）下降会推高单元件成本。
- 两个趋势叠加后，在任意时间点存在一个**成本最优的集成度**（minimum cost per component）。
- 1965 年时，这个最优点大约在每芯片 50 个元件；Moore 预测到 1970 年将达到约 1,000 个，到 1975 年达到 65,000 个。

**物理可行性论证**

Moore 用具体的几何计算论证了 65,000 元件的物理可行性：

- 当时的工艺公差下，高性能晶体管可以在 2 mil（约 50 μm）间距上制造。
- 这意味着每线性英寸至少 500 个元件，每平方英寸约 250,000 个元件。
- 因此 65,000 个元件仅需约 1/4 平方英寸的面积，在当时直径已达 1 英寸以上的 wafer 上完全可容纳。
- 多层金属互连（multilayer metallization）技术已经在研发中，可以解决布线密度问题。
- 不需要 electron beam 等更先进的光刻技术，现有光学光刻即可实现。

**散热问题的回应**

Moore 预判了一个关键质疑——数万元件集成在一起的散热问题：

- 集成电路是二维结构，每个热源附近都有表面可用于散热。
- 功耗主要用于驱动互连线和电容，当功能被约束在芯片上的小面积内时，需要驱动的电容量有限。
- 关键论点：**尺寸缩小使得在相同的单位面积功耗下可以获得更高的工作速度**——这实际上预示了后来 Dennard Scaling 的核心思想。

**良率提升的可行性**

Moore 认为实现 100% 良率没有根本性的物理障碍，不同于化学反应中的热力学平衡限制。当时良率不高主要是因为封装成本远超芯片本身成本，缺乏经济激励去优化良率。随着集成度提升使芯片价值增加，良率优化的投入将变得经济上合理。

**应用前景展望**

Moore 对集成电路的应用做出了若干预测：

- 家用计算机（或连接到中央计算机的终端）
- 汽车自动控制
- 个人便携通信设备
- 电子手表（仅需解决显示问题）
- 电话通信中的数字滤波和交换
- 分布式存储架构的计算机

同时他也指出了集成电路在 linear circuit 领域的局限：大电容和大电感本质上需要在体积中存储能量，与集成电路追求小体积的目标天然矛盾。

### 与现有工作的区别

这篇文章的独特性在于它不是一篇与其他工作对比的学术论文，而是一篇产业前瞻文章。但可以从以下角度理解它的定位：

- **相比于 Jack Kilby（TI）和 Robert Noyce（Fairchild）关于集成电路发明的工作**：Moore 的贡献不在于技术发明本身，而在于对技术演进速率的定量预测和经济学分析框架。
- **相比于当时的薄膜技术和微组装路线**：Moore 明确押注 semiconductor integrated circuit 路线，并给出了令人信服的经济性论证。
- **相比于后续 1975 年的修正版**：1965 年版的预测是每年翻倍；Moore 在 1975 年将其修正为每两年翻倍，这才是我们今天通常引用的 "Moore's Law" 版本。

## 实验评估

### 实验设置

本文不是传统意义上的实验论文，没有仿真平台或 benchmark。Moore 的"实验"是对 1959–1965 年间集成电路产业数据的回顾性分析：

- **数据来源**：Fairchild Semiconductor 及行业的生产数据
- **核心度量**：minimum cost per component 对应的集成度（components per integrated circuit）
- **时间跨度**：1959–1965 年的历史数据 + 向 1975 年的外推

### 关键结果

1. **集成度增长速率**：最低成本对应的单芯片元件数以大约每年翻一倍的速率增长（1959–1965 年的数据拟合）。
2. **成本下降预测**：到 1970 年，单元件制造成本将降至 1965 年水平的 1/10。
3. **1975 年预测**：按照每年翻倍的速率，到 1975 年最优集成度将达到 65,000 元件/芯片。
4. **物理可行性**：65,000 元件仅需约 1/4 平方英寸的面积，在当时的 wafer 尺寸和光刻精度下可行。

### 结果分析

Moore 的预测在后续几十年中被证明惊人地准确，尽管增长速率后来被修正为每两年翻倍。这一经验定律成为半导体产业规划的基石，不仅是一个观察结果，更演化为一个自我实现的产业目标——ITRS（International Technology Roadmap for Semiconductors）和后续的 IRDS 长期以 Moore's Law 为导向制定技术路线图。

值得注意的是，Moore 的分析框架以经济最优点而非技术极限为导向，这使得他的预测具有更强的鲁棒性——技术突破可以重新定义成本曲线，但经济驱动力保持不变。

## 审稿人视角

> 注：以现代顶会审稿标准评价一篇 1965 年的产业前瞻文章显然不完全公平，以下评价兼顾其历史贡献和方法论。

### 优点

1. **洞察力极为深刻**：Moore 将一个技术趋势外推为定量的经济规律，并准确预见了此后数十年半导体产业的发展轨迹。从 DRAM 的角度看，这篇文章虽然没有直接讨论存储器架构，但 Moore's Law 直接驱动了 DRAM 从 1Kbit（1970, Intel 1103）到今天数十 Gbit 单 die 的密度增长。这一趋势也是 DRAM memory wall 问题的根源——存储密度的增长远快于访问速度的提升。

2. **经济学视角的引入极具前瞻性**：Moore 没有单纯从物理极限出发，而是以 cost per component 为核心度量，将技术发展与经济可行性紧密结合。这种思维方式对后来的体系结构研究有深远影响——我们今天讨论 DRAM 架构设计时，同样需要在性能、功耗、面积成本之间做 trade-off，而非单纯追求性能极限。

3. **对关键质疑的系统性回应**：散热问题、良率问题、互连问题——Moore 逐一给出了定性但有说服力的论证，展现了优秀的工程直觉。他关于"缩小尺寸可以在相同功耗密度下提高速度"的论述，实际上预见了 1974 年 Dennard 提出的 Dennard Scaling 理论。

4. **应用预测极为准确**：家用计算机、个人通信设备、汽车电子——这些在 1965 年听起来像科幻的预测，在此后 20–40 年间全部实现。

### 不足

1. **缺乏严格的统计分析**：从方法论角度看，Moore 仅基于 6–7 个数据点（1959–1965）做 10 年外推，没有给出置信区间或误差分析。虽然结果证明预测基本正确，但从科学方法论的角度，这种外推的可靠性论证是不充分的。

2. **对 linear circuit 和 analog 领域的讨论不够深入**：Moore 指出大电容和大电感与集成化天然矛盾，但没有深入探讨 mixed-signal 集成的可能路径。实际上，模拟/混合信号集成后来走出了自己的技术路线。

3. **未考虑互连延迟的 scaling 问题**：Moore 乐观地认为多层金属互连可以解决布线问题，但没有预见到互连延迟随尺寸缩小反而恶化的趋势（即后来的 interconnect scaling crisis），这在亚微米时代成为主要瓶颈之一。

4. **对功耗问题的论述过于乐观**：Moore 关于散热的论证在 Dennard Scaling 有效的时代是正确的，但没有预见到 Dennard Scaling 在 2000 年代中期失效后出现的 power wall 问题。当然，要求一篇 1965 年的文章预见 40 年后的问题是不合理的。

### 疑问或值得追问的点

- **Moore's Law 对 DRAM 技术的特殊影响**：DRAM 作为密度驱动型器件，是 Moore's Law 最直接的受益者之一。但 DRAM 的 scaling 也面临独特的挑战——存储电容随尺寸缩小而减小，这导致了从 planar capacitor 到 trench capacitor 再到 stacked capacitor 的结构演进，以及近年来向 3D DRAM 的转型探索。Moore 的原始分析框架是否足以捕捉这种结构性变革带来的非线性效应？

- **从 Moore's Law 到 Memory Wall**：Moore's Law 驱动了处理器和存储器密度的共同增长，但两者的速度增长却严重失衡。Wulf & McKee 在 1995 年提出的 Memory Wall 问题，某种意义上正是 Moore's Law 在不同维度上差异化体现的必然结果。这是否意味着 Moore 的单一维度（元件数/芯片）分析框架从一开始就忽略了系统层面的 balance 问题？

- **后 Moore 时代的启示**：当传统 scaling 趋近物理极限时，chiplet、3D 封装、near-memory computing 等"超越 Moore"的技术路线正在重新定义"集成"的含义。Moore 在 1965 年将集成定义为"在单一衬底上"，而今天的异构集成已经突破了这一边界。Moore 的经济学分析框架（cost per function）在 chiplet 时代如何适用，是一个有趣的开放问题。
